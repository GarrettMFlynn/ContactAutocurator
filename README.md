# ContactAutocurator
Code to train and deploy neural network models to curate whisker touch data

[![Hires Lab](https://github.com/jonathansy/whisker-autocurator/blob/master/Resources/Images/HiresLab-logoM.png)](http://68.181.113.239:8080//hireslabwiki/index.php?title=Main_Page)

[![License: GPL v2](https://img.shields.io/badge/License-GPL%20v2-blue.svg)](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)

## Introduction
ContactAutocurator is a pipeline for curating contact data using a convolutional neural network (CNN). It was originally designed to work on videos of mice touching a pole, however it can likely be generalized to other kinds of contacts. It was developed in conjunction with the Janelia Farm whisker tracker, however, the whisker tracking is used strictly for preprocessing and not necessary for autocuration to occur. The code base contains several pre-trained CNN models as well as a pipeline to easily implement autocuration in MATLAB. For best results on different types of video data, it is preferable to train a new model from manually curated data. As such, a training pipeline is included to create custom CNN models from labeled images. 

Currently the pipeline only supports MATLAB, however intermediary steps call Python code utilizing numpy arrays. A Python-only pipeline is thus possible, but would require rewriting the preprocessing and post-processing steps. One can also feed the numpy arrays generated by the autocurator into a custom Python dataset. 

The convolutional neural networks are designed to be trained and deployed on the cloud. While nothing about the model training and deployment code are platform-limiting, the pipeline was written to use Google Cloud Platform (CloudML) and the installation instructions will contain a brief guide to setting up a Google Cloud Platform account. If you have access to a CUDA-enabled GPU, local versions of the neural network code are included but it is highly recommended that they are not used without a GPU. 



------
## Installation 
The cloud training and curation scripts in this account have all been written for Google Cloud Platform's cloud ML API. 

### Dependencies
* [Python 3.5 or higher](https://www.python.org/downloads/) (compatibility not tested with 2.7)  
  - Numpy package  
  - Pickle package  
  - Scipy package
* [Google Cloud SDK package](https://cloud.google.com/sdk/)
* MATLAB r2013b or later
  - [npy-matlab](https://github.com/kwikteam/npy-matlab)
* Google Cloud Platform Account



### Recommended
* [Janelia Farm Whisker Tracker](https://wiki.janelia.org/wiki/display/MyersLab/Whisker+Tracking+Downloads) (all distance-to-pole preprocessing assumes you have it)

* MATLAB Packages (for Hires Lab)
  - [HLab_MatlabTools](https://github.com/hireslab/HLab_MatlabTools)  
  - [HLab_Whiskers](https://github.com/hireslab/HLab_Whiskers)



Cloud ML Setup
------
If you intend to use the cloud curation scripts in this package, you will first need to setup a consistent directory structure within a Google Cloud Bucket. Buckets are a form of cloud data storage. Information about creating and using them can be found [here](https://cloud.google.com/storage/docs/creating-buckets). After creating a cloud storage bucket for your training jobs, you should create the following directories:
/Jobs for storing output logs from curation
/Data for importing uncurated image data 
/Curated_Data for placing curated labels for export 
/Model_Saves for placing the model(s) you wish to use for curation.

Cloud storage buckets begin with the prefix gs://, for example gs://my_bucket/Data. Make sure all relevant cloud paths are set within [autocurator_master_function.m](https://github.com/jonathansy/whisker-autocurator/blob/master/Autocurator_Beta/autocurator_master_function.m). Cloud storage buckets do not have the same functionality as local drives and thus will not display their properties or index files. They also require the File_IO package to index on Python (included with Tensorflow). 

Submitting training jobs to CloudML do not require manually setting up virtual machines on Google's cloud console. You will have to specify certain settings for the curation environment when you submit a job to the cloud. These are handled via  the 'gcloud ml-engine jobs submit' command as well as a .yaml file included in the local directory. Important variables to specify are the type of GPU (if any) to use, the runtime environment, and the [region](https://cloud.google.com/compute/docs/regions-zones/) (note that only certain regions support GPU use).    

Local Drive Setup 
------
Autocurator_Beta, HLab_MatlabTools, HLab_Whiskers, and npy-matlab should all be cloned to the local Github location and added to the MATLAB path. Python should be installed. Once Python is installed, use pip to install the other packages on the command line (exact syntax for using pip may vary with your version of Python). Install Google Cloud SDK and make sure its commands can be run from the command line prompt. 

Within Autocurator_Beta, make sure you have a subdirectory called 'trainer' (you can rename it but you will need to change the pathing in 'autocurator_master_function.m'. Within this subdirectory you should have cnn_curator_cloud.py, the .yaml file you are using as a configuration file (default/example included in this package), and \_\_init\_\_.py (which is an empty Python file but required for operation). 

You should designate a directory on your local drive to save .npy files as well as a location to save the final curated datasets. 

------
## Usage

ContactAutocurator is a pipeline and neural network model designed to read a session of videos and determine the contact times in those videos. It was designed to determine when a mouse whisker is touching a pole, but the pipeline itself should be generalizable for other types of tactile contacts. With that in mind (and because video data can vary dramatically based on the experiment), the ContactAutocurator code contains two main pipelines. One is to train a new convolutional neural network model with pre-curated training data. The other pipeline will curate new data based on a selected model. 

Because the pipelines were developed with whisker curation in mind, they assume that you have the Janelia Farm Whisker tracker or a similar method of pre-tracking as well as a method of cropping videos to the regions where contacts are occuring (while not strictly necessary, this dramatically lowers the size of the processed images, thus increasing speed and simplifying the necessary model). If you do not have the Janelia Farm Whisker tracker, you will either need to recode the pipeline for preprocessing (based on the contacts you want to detect) or disable preprocessing entirely. 

For more in depth reading on each pipeline see:
* [Model Training Pipeline](https://github.com/jonathansy/ContactAutocurator/blob/master/docs/Training_Pipeline_Documentation.md)
* [Autocurator Pipeline](https://github.com/jonathansy/ContactAutocurator/blob/master/docs/Autocurator_Documentation.md) 
